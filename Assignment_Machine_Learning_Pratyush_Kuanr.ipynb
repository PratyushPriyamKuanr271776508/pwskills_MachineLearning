{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP9U0oFV41+HB9DgaFcf0yN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PratyushPriyamKuanr271776508/pwskills_MachineLearning/blob/main/Assignment_Machine_Learning_Pratyush_Kuanr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. What is a parameter?**\n",
        "A parameter is a variable that the model learns during training to make predictions. Examples include weights and biases in linear regression."
      ],
      "metadata": {
        "id": "nwWi414d-UDl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. What is correlation?**\n",
        "Correlation is a statistical measure that indicates the degree to which two variables move in relation to each other. It ranges from -1 to +1."
      ],
      "metadata": {
        "id": "pSILUIMz-kok"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. What does negative correlation mean?**\n",
        "Negative correlation means that as one variable increases, the other decreases. For example, higher temperatures may correlate with lower heating costs."
      ],
      "metadata": {
        "id": "Jt0F7Jug-oew"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. Define Machine Learning. What are the main components in Machine Learning?**\n",
        "**Definition**: Machine Learning is a field of AI that enables systems to learn and improve from experience without being explicitly programmed.\n",
        "\n",
        "**Main components**:\n",
        "- **Data**: Input for the model.\n",
        "- **Model**: The algorithm to learn from data.\n",
        "- **Loss Function**: Measures prediction error.\n",
        "- **Optimizer**: Minimizes the loss function.\n",
        "- **Evaluation**: Assess model performance.\n"
      ],
      "metadata": {
        "id": "6jU-2cY--p-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5. How does loss value help in determining whether the model is good or not?**\n",
        "A lower loss value typically indicates better model performance, as it represents the difference between predicted and actual values."
      ],
      "metadata": {
        "id": "x1_B_m8T-wWI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **6. What are continuous and categorical variables?**\n",
        "- **Continuous variables**: Numerical values with infinite possibilities (e.g., age, temperature).\n",
        "- **Categorical variables**: Discrete values or categories (e.g., gender, color)."
      ],
      "metadata": {
        "id": "dNrWGj_4-1iF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **7. How do we handle categorical variables in Machine Learning? What are the common techniques?**\n",
        "**Techniques**:\n",
        "1. **Label Encoding**: Assigns numerical labels to categories.\n",
        "2. **One-Hot Encoding**: Creates binary columns for each category.\n",
        "3. **Ordinal Encoding**: Assigns ordinal values based on category hierarchy.\n"
      ],
      "metadata": {
        "id": "omDF_mVS-4Vi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **8. What do you mean by training and testing a dataset?**\n",
        "Training is used to fit the model, while testing evaluates its performance on unseen data."
      ],
      "metadata": {
        "id": "ufz36Bse-9dX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **9. What is sklearn.preprocessing?**\n",
        "`sklearn.preprocessing` is a module in scikit-learn providing tools for preprocessing data, such as scaling, encoding, and normalizing.\n"
      ],
      "metadata": {
        "id": "i4tq0S-c_AIT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **10. What is a test set?**\n",
        "A test set is a subset of data reserved for evaluating the model's performance after training."
      ],
      "metadata": {
        "id": "X7biYu2N_DFL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **11. How do we split data for model fitting (training and testing) in Python?**"
      ],
      "metadata": {
        "id": "YSKhsZk0_IQz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CicqPCKr-S6B"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = [[1], [2], [3], [4], [5]]\n",
        "y = [1, 2, 3, 4, 5]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **12. How do you approach a Machine Learning problem?**\n",
        "Steps:\n",
        "1. Understand the problem.\n",
        "2. Collect and preprocess data.\n",
        "3. Perform Exploratory Data Analysis (EDA).\n",
        "4. Select a suitable model.\n",
        "5. Train and validate the model.\n",
        "6. Evaluate and optimize.\n",
        "7. Deploy and monitor.\n"
      ],
      "metadata": {
        "id": "rADSlFG0_Mik"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **13. Why do we have to perform EDA before fitting a model to the data?**\n",
        "EDA helps understand data distribution, detect outliers, and reveal patterns that influence model selection and preprocessing."
      ],
      "metadata": {
        "id": "MUzh0hp2_SQL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **14. How can you find correlation between variables in Python**\n",
        "\n",
        "#### **Using Pandas**\n",
        "The `corr()` function in Pandas calculates the correlation matrix, which shows the pairwise correlation between variables.\n",
        "\n",
        "#### **Using Numpy**\n",
        "You can use `numpy.corrcoef()` for correlation computation.\n",
        "\n",
        "### **Visualization with Seaborn**\n",
        "To visualize the correlation matrix:"
      ],
      "metadata": {
        "id": "44Nb7tzlBRAM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **15. What is causation? Explain the difference between correlation and causation with an example.**\n",
        "**Causation**: One event directly affects another.\n",
        "**Difference**: Correlation indicates association, while causation shows a cause-effect relationship.\n",
        "\n",
        "**Example**: Ice cream sales and drowning incidents are correlated (summer), but one does not cause the other."
      ],
      "metadata": {
        "id": "a7xUQKNq_Uuh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **16. What is an Optimizer? What are different types of optimizers?**\n",
        "An optimizer adjusts model parameters to minimize the loss function.\n",
        "\n",
        "**Types**:\n",
        "- Gradient Descent\n",
        "- Stochastic Gradient Descent (SGD)\n",
        "- Adam"
      ],
      "metadata": {
        "id": "yIH41fUZ_Y9x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "optimizer = Adam(learning_rate=0.001)"
      ],
      "metadata": {
        "id": "hIkDuDwRB-yz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **17. What is sklearn.linear_model?**\n",
        "`sklearn.linear_model` provides tools for linear models such as Linear Regression, Logistic Regression, etc."
      ],
      "metadata": {
        "id": "LqPGkjnL_bo9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **18. What does model.fit() do?**\n",
        "Fits the model to training data.\n",
        "\n",
        "**Arguments**:\n",
        "- Features (X)\n",
        "- Target labels (y)"
      ],
      "metadata": {
        "id": "kcZAXZPZ_dcv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **19. What does model.predict() do?**\n",
        "Makes predictions on new data.\n",
        "\n",
        "**Arguments**:\n",
        "- Input data (X)"
      ],
      "metadata": {
        "id": "7icDK9sz_fba"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **20. What is feature scaling? How does it help in Machine Learning?**\n",
        "**Definition**: Normalizing feature values to a standard scale.\n",
        "\n",
        "**Helps**: Prevents features with large values from dominating."
      ],
      "metadata": {
        "id": "I5M2JcQ5_hsW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Scaling Techniques**\n",
        "1. **Min-Max Scaling**: Scales features to a fixed range, typically [0, 1].\n",
        "2. **Standardization (Z-Score Scaling)**: Scales data to have a mean of 0 and a standard deviation of 1.\n",
        "3. **MaxAbs Scaling**: Scales features by their maximum absolute value.\n",
        "4. **Robust Scaling**: Scales features using the median and interquartile range (IQR), making it robust to outliers.\n",
        "\n",
        "### **When to Use Which Scaling Method?**\n",
        "- Use **Min-Max Scaling** for algorithms sensitive to ranges (e.g., neural networks).\n",
        "- Use **StandardScaler** for algorithms assuming normal distributions (e.g., SVM, PCA).\n",
        "- Use **RobustScaler** when data has outliers.\n"
      ],
      "metadata": {
        "id": "-suNhTqcCs7K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### 1. Using sklearn.preprocessing.StandardScaler (Standardization)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# Example data\n",
        "data = np.array([[1.0, 2.0], [2.0, 4.0], [3.0, 6.0]])\n",
        "\n",
        "# Standardization\n",
        "scaler = StandardScaler()\n",
        "data_scaled = scaler.fit_transform(data)\n",
        "\n",
        "print(\"Standardized Data:\\n\", data_scaled)\n",
        "\n",
        "#### 2. Using sklearn.preprocessing.MinMaxScaler (Min-Max Scaling)\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Example data\n",
        "scaler = MinMaxScaler()\n",
        "data_scaled = scaler.fit_transform(data)\n",
        "\n",
        "print(\"Min-Max Scaled Data:\\n\", data_scaled)\n",
        "\n",
        "#### 3. Using sklearn.preprocessing.MaxAbsScaler\n",
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "\n",
        "scaler = MaxAbsScaler()\n",
        "data_scaled = scaler.fit_transform(data)\n",
        "\n",
        "print(\"MaxAbs Scaled Data:\\n\", data_scaled)\n",
        "\n",
        "#### 4. Using sklearn.preprocessing.RobustScaler\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "scaler = RobustScaler()\n",
        "data_scaled = scaler.fit_transform(data)\n",
        "\n",
        "print(\"Robust Scaled Data:\\n\", data_scaled)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mo9p5EmgC9NI",
        "outputId": "87dca5ba-ed61-4ec3-f45b-d9b1061e6975"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Standardized Data:\n",
            " [[-1.22474487 -1.22474487]\n",
            " [ 0.          0.        ]\n",
            " [ 1.22474487  1.22474487]]\n",
            "Min-Max Scaled Data:\n",
            " [[0.  0. ]\n",
            " [0.5 0.5]\n",
            " [1.  1. ]]\n",
            "MaxAbs Scaled Data:\n",
            " [[0.33333333 0.33333333]\n",
            " [0.66666667 0.66666667]\n",
            " [1.         1.        ]]\n",
            "Robust Scaled Data:\n",
            " [[-1. -1.]\n",
            " [ 0.  0.]\n",
            " [ 1.  1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **21. Explain data encoding.**\n",
        "Data encoding converts categorical variables into numerical forms, such as Label Encoding or One-Hot Encoding."
      ],
      "metadata": {
        "id": "FmGrYQ-u_qPZ"
      }
    }
  ]
}